{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbd97111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Set LangSmith Environment Variables\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter API key for LangSmith\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77c0a804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing chat model!\n",
      "Chat model initialization over!\n"
     ]
    }
   ],
   "source": [
    "# Set up google llm model for RAG usage\n",
    "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "\n",
    "print(\"Initializing chat model!\")\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "print(\"Chat model initialization over!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d23c2913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedder model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/rag-agent-musings/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedder model initialized!\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing embedder model!\")\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "print(\"Embedder model initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35bbca89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize vector store!\n",
      "Vector store initialized!\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialize vector store!\")\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "print(\"Vector store initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b29c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Loading documents for RAG use\n",
    "\n",
    "# get beautiful soup\n",
    "import bs4\n",
    "# get Document Loaders\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "desired_webpage_link = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "# the original tutorial only wanted us to keep html tags that\n",
    "# contain any of the class names listed in the tuple assigned to the \n",
    "# class_ keyword, in the below variable. We discard all other html tags.\n",
    "bs4_filtered = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "\n",
    "webpage_loader = WebBaseLoader(\n",
    "  web_paths=(desired_webpage_link,),\n",
    "  bs_kwargs={\"parse_only\": bs4_filtered}\n",
    ")\n",
    "\n",
    "documents = webpage_loader.load()\n",
    "#for key in documents[0]:\n",
    "  #print(f\"New key: {key}\")\n",
    "#print(f\"how many characters are in our page: {len(documents[0].page_content)}\")\n",
    "#print(f\"The first two thousand characters of the document: {documents[0].page_content[:2000]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d03c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# create a text splitter instance\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=1000,\n",
    "  chunk_overlap=200,\n",
    "  add_start_index=True,\n",
    ")\n",
    "\n",
    "# obtain splitted text. our document is now a bunch of document shards.\n",
    "splitted_text = recursive_text_splitter.split_documents(documents)\n",
    "#print(f\"The splits we got back:\\n\\n{splitted_text[4].page_content}\")\n",
    "#print(f\"Number of splits:{len(splitted_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f49a315",
   "metadata": {},
   "source": [
    "# We've loaded in our document. \n",
    "Split it up into text chunks, to allow our model to access bits of the data that fit into the LLM context window.\n",
    "\n",
    "Now, it's time to put these chunks into a storage format the LLM can access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70699a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the vector store was previously initialized with our defined embeddings\n",
    "# in one command sequence, we tell the vector store to embed each of our document chunks.\n",
    "# we then store the embedded versions of these chunks into our vector store, for \n",
    "# later retrieval\n",
    "vector_store__document_embedded_vectors = vector_store.add_documents(splitted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3de96322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector store shape: ['eb1ac7c2-aa0c-4f91-a757-cea1955480e9', 'bfa8c2ca-9ed3-41ed-8d2b-7b145636c945', '1cb01e59-353a-48f0-a24f-0a37c4a135ec', '965ab726-e531-4b8e-a5d0-5154e0c13246', '2a8c61ea-1bd9-4a57-9d36-1ec73db8dfe9', '1fd8222f-b269-4fcf-8d68-0f45bb87e17d', '62562bbf-7ccb-49aa-8255-7e8c87b1c4fc', '3cb5ede2-bc8a-41fc-b272-d5eab1a9e6b6', 'dd8688e8-c300-45c6-8e00-5e5f649547a3', 'b828027c-ecdc-4945-98ef-77cfddd7ad10', 'e1d66edc-244e-44a8-a3fd-1c52ffa5d9aa', '9fb6f349-f66c-4ee4-8838-06cd7648baf0', '0fd59855-2a03-45b3-a31a-a788ba9391cf', '7a2ef340-8005-49bb-9dfa-441f5891e5f1', '2287ecd1-8806-464c-93d2-fa0d79ec41f3', '479adcf8-2acf-498b-b291-61752f08aff3', 'e95ac269-2c54-4a95-9a04-b21666a51831', '7e0018c4-2407-465e-8531-bbc7717d0c4e', '376181ec-27f8-481a-b713-cae027a2b7c2', '3dffb694-9f85-4cbe-9de8-ccc164bce118', '1468d411-38bc-414b-b348-1707f5d1f68d', 'bda7cba4-fba6-4897-9604-7c00964d3a76', '1ef7f8df-9cc5-4d06-9a58-c2730919f945', '8bf7f7d6-2e51-4dd2-8e1d-8ec7a970cb3a', '6c1b54f8-1248-42a8-8e3c-95db0b8b1b9a', '93696a85-48df-48be-94ae-7f18310a4aa7', 'e88b918b-a815-4378-bfa2-7298e8f7b90e', '7c217e46-2316-4a28-8a95-b2e50d47825d', 'cdf4b986-9d3e-4bdc-9be0-5f3b6ff15ff5', 'c6433a0b-7448-4041-bf7c-8a61d0f7c4c2', '9ef5be98-a3fe-4595-98e6-0bb54a098d2d', '17fc2107-a3dd-4abd-9a9f-65f0be482e23', 'ad1d055c-0703-42c2-be89-507840b1d9cb', 'df75a898-3a51-47ad-ad10-4a09fc331a42', 'e8495db6-d4f2-4978-b9e3-c8c062a7d5fc', 'd22348a9-4746-4c88-9b31-a97b0ce6b559', 'ccfaf2cd-2815-42e6-80f9-c542378c48ac', '9b54d71e-5aeb-4d55-8c66-1f9a9770eb58', '70e065c8-06e1-41d8-8c2e-7ba4f307fa9b', 'f04a25f3-6b3e-471d-8141-aade93758091', '4b8d28c8-1dfc-49fb-ada9-b746f3217c6d', '4bde0d7a-1a45-4aa5-98f6-02216bee0fa9', 'fa97a8e7-7729-42a1-a38d-1e9b33535b4e', '51e55ccd-88f0-4b51-9f9c-eb5ace56e1cd', '9f745613-1ec3-4119-aeb8-6384639b30a8', '86057bc6-4157-4679-a792-17b6b85f2898', '0ed3154e-c054-4950-b104-fc31af375f60', 'f4c77b6e-e8ed-480a-b33a-4c484e4755d4', '8b6cc25b-a634-40d4-b616-7e38a55d6eff', '2c2e25d3-c46a-416a-9c40-5eb37a181740', '9ee22449-3791-4ff1-a37b-7606e6211025', '9ccdfa09-bffb-4524-a93d-0b3fba9a1aaa', '609d2ff9-f531-4ed9-84f6-178e010e75eb', 'cb6a4e33-aa7f-4cbc-9ffb-b17c8377efff', '33fbcdb4-15ca-404b-bce9-e393dc8531ce', '27918244-54d6-49b1-a51a-a7366d9b7f7f', '8338f37e-d434-4f7d-aa50-6481e789898d', '4bd5defd-8ff2-4e0a-9414-1466da7b7019', '04174804-2a78-4ffd-9198-9488ebe45a6e', '0573c231-f01f-413b-a964-2ee4fe97b861', '806aaddf-ac98-4845-a1f3-dd090c7e905a', '3a471341-3c68-46bb-af8c-89d4f327af99', 'c47b66bd-364f-4e4d-a45e-7bd4134c17c4']\n"
     ]
    }
   ],
   "source": [
    "print(f\"vector store shape: {vector_store__document_embedded_vectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7784050",
   "metadata": {},
   "source": [
    "# We now should be able to answer User Queries\n",
    "By accessing our vector store and returning the relevant chunks of text, from Lillian's blog post, that answers the user's questions.\n",
    "\n",
    "## Onto Retrieval And Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ad84b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example rag message: [HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: question here \\nContext: context here \\nAnswer:\", additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Get RAG prompt\n",
    "from langchain import hub\n",
    "\n",
    "# The actual RAG prompt\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "#print(f\"Actual template response: {rag_prompt.messages[0].prompt.template}.\")\n",
    "\n",
    "example_rag_message = rag_prompt.invoke({\n",
    "  \"context\": \"context here\",\n",
    "  \"question\": \"question here\"\n",
    "}).to_messages() # Example of passing in context and question to a full RAG prompt.\n",
    "\n",
    "print(f\"Example rag message: {example_rag_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6641d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use LangGraph to generate for our RAG application.\n",
    "\n",
    "# Begin the state definition phase, where we discuss what type of data we want our RAG model to process.\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Create the actual state, to define what data our RAG model handles\n",
    "\n",
    "class RAG_State(TypedDict):\n",
    "  question: str\n",
    "  context: List[Document] # remember, context should be several chunks of a document\n",
    "  answer: str\n",
    "\n",
    "def retrieve(rag_state: RAG_State):\n",
    "  \"\"\"\n",
    "  Retrieves relevant data, based on our user question, from the vector store\n",
    "\n",
    "  Args:\n",
    "    rag_state: A dictionary which, for this problem, contains the user question, in the key 'question'. Has to be of type RAG_State\n",
    "  \n",
    "  Returns:\n",
    "    All relevant context document chunks. Should be a dictionary of the following form: {str : List[Document]}\n",
    "  \"\"\"\n",
    "  similar_document_chunks = vector_store.similarity_search(rag_state[\"question\"])\n",
    "  return {\"context\" : similar_document_chunks}\n",
    "\n",
    "def generate(rag_state: RAG_State):\n",
    "  \"\"\"\n",
    "  Generate appropriate RAG response to the user query.\n",
    "\n",
    "  Args:\n",
    "    rag_state: A dictionary which, for this problem, contains the user question, in the key 'question', and the context of the question, in the key 'context'. Has to be of type RAG_State\n",
    "  \n",
    "  Returns:\n",
    "    The model's response. Should be a dictionary of the following form: {str: ?}\n",
    "  \"\"\"\n",
    "  documents_contents = \"\\n\\n\".join(doc.page_content for doc in rag_state[\"context\"])\n",
    "  formalized_RAG_prompt = rag_prompt.invoke({\"question\": rag_state[\"question\"], \"context\": rag_state[\"context\"]})\n",
    "  llm_RAG_response = llm.invoke(formalized_RAG_prompt)\n",
    "  return {\"answer\" : llm_RAG_response.content}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-agent-musings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
