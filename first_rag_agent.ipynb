{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbd97111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Set LangSmith Environment Variables\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter API key for LangSmith\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77c0a804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing chat model!\n",
      "Chat model initialization over!\n"
     ]
    }
   ],
   "source": [
    "# Set up google llm model for RAG usage\n",
    "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "\n",
    "print(\"Initializing chat model!\")\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "print(\"Chat model initialization over!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d23c2913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedder model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/rag-agent-musings/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedder model initialized!\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing embedder model!\")\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "print(\"Embedder model initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35bbca89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize vector store!\n",
      "Vector store initialized!\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialize vector store!\")\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "print(\"Vector store initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b29c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Loading documents for RAG use\n",
    "\n",
    "# get beautiful soup\n",
    "import bs4\n",
    "# get Document Loaders\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "desired_webpage_link = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "# the original tutorial only wanted us to keep html tags that\n",
    "# contain any of the class names listed in the tuple assigned to the \n",
    "# class_ keyword, in the below variable. We discard all other html tags.\n",
    "bs4_filtered = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "\n",
    "webpage_loader = WebBaseLoader(\n",
    "  web_paths=(desired_webpage_link,),\n",
    "  bs_kwargs={\"parse_only\": bs4_filtered}\n",
    ")\n",
    "\n",
    "documents = webpage_loader.load()\n",
    "#for key in documents[0]:\n",
    "  #print(f\"New key: {key}\")\n",
    "#print(f\"how many characters are in our page: {len(documents[0].page_content)}\")\n",
    "#print(f\"The first two thousand characters of the document: {documents[0].page_content[:2000]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d03c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# create a text splitter instance\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=1000,\n",
    "  chunk_overlap=200,\n",
    "  add_start_index=True,\n",
    ")\n",
    "\n",
    "# obtain splitted text. our document is now a bunch of document shards.\n",
    "splitted_text = recursive_text_splitter.split_documents(documents)\n",
    "#print(f\"The splits we got back:\\n\\n{splitted_text[4].page_content}\")\n",
    "#print(f\"Number of splits:{len(splitted_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f49a315",
   "metadata": {},
   "source": [
    "# We've loaded in our document. \n",
    "Split it up into text chunks, to allow our model to access bits of the data that fit into the LLM context window.\n",
    "\n",
    "Now, it's time to put these chunks into a storage format the LLM can access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70699a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the vector store was previously initialized with our defined embeddings\n",
    "# in one command sequence, we tell the vector store to embed each of our document chunks.\n",
    "# we then store the embedded versions of these chunks into our vector store, for \n",
    "# later retrieval\n",
    "vector_store__document_embedded_vectors = vector_store.add_documents(splitted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3de96322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector store shape: ['3c76c953-85d1-4259-a476-1d2dd62a49ef', '0423c64b-bf95-49d9-acb5-fbbe88396bc9', '85e3f792-b3d3-4232-bdb7-ad6aca97cae6', 'ca24eaff-32c1-4288-be45-096e5ab6c3ff', '23a933a5-4fb7-4f28-b063-65d6b0a5d49c', 'aa2ca783-7b85-4ce8-813a-134551ad2188', '230b992a-9fb6-409d-ae84-07d0443c4549', '4a807761-bc9c-41dd-8813-d6a6bfc8d766', '417b929f-23e3-4cc8-8d44-824a4d4bc3af', '84b02693-64a3-4022-b94f-eb21bdc872cf', '54c246db-e38f-4a8a-8a99-4994e6875c64', '71a6923b-0fd0-4333-a734-2ae292c703ee', 'a5c38ce7-301c-4aac-991a-6a0603c7ae83', 'fa9e5f14-8049-424c-9414-163270f5c12d', '33a740b9-86b3-4ee4-a454-bc067092d3b7', '17e95539-2d28-4ce1-baba-814933724128', 'c2da44d5-4a93-40ef-9f7e-34adea180c6d', 'dec193b1-3dc9-46f9-882b-2d7525210cb8', 'fb5dfe54-ba31-49d6-83ce-ebfc1191b19d', '4c986bf8-f0d3-4711-a207-db84a9b8cbf1', 'f11b6967-c3c7-4a18-83bc-df296e58b0a1', '42a81ee6-4e75-4c81-ab83-cd72ce42cb41', 'e26bc43d-a23e-4e57-bda2-a8a02bde9a8c', 'a02f7c21-d607-4bc8-939e-33564a311508', 'f8df3097-ceeb-4e66-aa36-e3f80dc52a2b', 'f183474f-b78f-44b9-a52c-82810a97899d', '8f44fca1-6e09-4cef-bb35-26ecf0eb5c4c', '6b014395-94b9-40ee-a540-9187c88e489f', '14eb39d4-eb22-4d1f-9cc7-4a9eef3f62d8', 'c74e7dd0-9fb8-4a52-823a-e3e9f738393d', '6c5b2fb5-2efd-4080-8612-87d451a9ec07', '7c65785b-6169-4691-bf0c-1284babaaf54', '33661822-3280-40a3-9520-6d3f91d68b4b', '186c3256-f46a-4cce-b69b-40ee0635d373', '2c433d83-752d-44d1-870d-ec06cc4635db', '72f072c9-1314-4d71-b782-63eddc404db7', 'aefc7c9a-4826-4a14-b005-3604cd7ff71f', '077241a8-5702-40ab-b1b7-e01265b63b9d', 'de8d3c51-b766-4766-9551-49cd96bc1ffb', '87b964b3-6ef1-4b4f-8702-c41444aa870c', '843df63b-9fc9-45b3-b382-83f0e31a38d8', '119c1c7e-1661-4210-8ad7-c4565bc24b71', '0eb1b28d-3a09-4bc4-a250-deeddae6948d', '244ebffd-c46c-43b7-8ad1-fd5c2ad120c2', '28efabfb-c5f8-4a3b-8323-a8f33c7a93b4', '57a84628-2f97-4135-b627-f0f650757fb1', 'e5374171-9af0-49a7-9c28-9bf61c358cf1', '9ee0cdd7-04b3-493c-ac95-eba6465db18f', 'ca762ee0-ea13-4f67-ace0-7746d23e6425', '23bf4466-44dd-43b1-87a7-8474b5571549', 'cb652d6c-3d2e-4e81-af1e-f324b70d2a0b', 'ab311e87-d0c9-4233-8475-714387d59a6b', '96c0e9e5-e0ed-4419-9188-82a9f411620f', '9b5d22cc-dcd9-4c57-b552-6251d5c8b6dc', '9790c96d-4194-418f-aee4-34f9f1d1401b', '49f520b8-e184-41f9-8dcc-422fc1ded69f', '0b937d2e-1bfd-418b-8812-c42ad4086618', '85f33d4d-9313-4fcf-8d67-978abc45377a', 'd5584765-7a29-4c99-b0e3-b5db0bb54ec5', '1d023d67-ac6e-4b09-8ac5-8f905cb1f974', 'ef9cde20-b812-4a99-b8b0-ab38817844f1', 'c120c9bb-7817-4811-b349-d18d5c80d0b5', 'e3b7b03e-55df-40a5-a1fa-219fdd0eb9ce']\n"
     ]
    }
   ],
   "source": [
    "print(f\"vector store shape: {vector_store__document_embedded_vectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7784050",
   "metadata": {},
   "source": [
    "# We now should be able to answer User Queries\n",
    "By accessing our vector store and returning the relevant chunks of text, from Lillian's blog post, that answers the user's questions.\n",
    "\n",
    "## Onto Retrieval And Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ad84b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example rag message: [HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: question here \\nContext: context here \\nAnswer:\", additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Get RAG prompt\n",
    "from langchain import hub\n",
    "\n",
    "# The actual RAG prompt\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "#print(f\"Actual template response: {rag_prompt.messages[0].prompt.template}.\")\n",
    "\n",
    "example_rag_message = rag_prompt.invoke({\n",
    "  \"context\": \"context here\",\n",
    "  \"question\": \"question here\"\n",
    "}).to_messages() # Example of passing in context and question to a full RAG prompt.\n",
    "\n",
    "print(f\"Example rag message: {example_rag_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6641d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use LangGraph to generate for our RAG application.\n",
    "\n",
    "# Begin the state definition phase, where we discuss what type of data we want our RAG model to process.\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Create the actual state, to define what data our RAG model handles\n",
    "\n",
    "class RAG_State(TypedDict):\n",
    "  question: str\n",
    "  context: List[Document] # remember, context should be several chunks of a document\n",
    "  answer: str\n",
    "\n",
    "def retrieve(rag_state: RAG_State):\n",
    "  \"\"\"\n",
    "  Retrieves relevant data, based on our user question, from the vector store\n",
    "\n",
    "  Args:\n",
    "    rag_state: A dictionary which, for this problem, contains the user question, in the key 'question'. Has to be of type RAG_State\n",
    "  \n",
    "  Returns:\n",
    "    All relevant context document chunks. Should be a dictionary of the following form: {str ; List[Document]}\n",
    "  \"\"\"\n",
    "  similar_document_chunks = vector_store.similarity_search(rag_state[\"question\"])\n",
    "  return {\"context\" : similar_document_chunks}\n",
    "\n",
    "def generate(rag_state: RAG_State):\n",
    "  \"\"\"\n",
    "  Generate appropriate RAG response to the user query.\n",
    "\n",
    "  Args:\n",
    "    rag_state: A dictionary which, for this problem, contains the user question, in the key 'question', and the context of the question, in the key 'context'. Has to be of type RAG_State\n",
    "  \n",
    "  Returns:\n",
    "    The model's response. Should be a dictionary of the following form: {str ; ?}\n",
    "  \"\"\"\n",
    "  documents_contents = \"\\n\\n\".join(doc.page_content for doc in rag_state[\"context\"])\n",
    "  formalized_RAG_prompt = rag_prompt.invoke({\"question\": rag_state[\"question\"], \"context\": rag_state[\"context\"]})\n",
    "  llm_RAG_response = llm.invoke(formalized_RAG_prompt)\n",
    "  return {\"answer\" : llm_RAG_response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f20c0ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph object, to link together the retrieval and generate steps.\n",
    "# Note that our retrieval and generate steps are simple as of now, but can be\n",
    "# made more complex.\n",
    "\n",
    "from langgraph.graph import START, StateGraph # START is a special Start Node\n",
    "\n",
    "rag_agent_maker = StateGraph(RAG_State).add_sequence([retrieve, generate])\n",
    "rag_agent_maker.add_edge(START, \"retrieve\") # links the special Start node to the retrieve node sequence we defined above.\n",
    "rag_agent = rag_agent_maker.compile() # now we have made our RAG agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4792f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
      "\n",
      "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\n",
      "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agentâ€™s working memory, up to three, to be used as context for querying LLM.\n",
      "\n",
      "\n",
      "Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\n",
      "Answer: Optimizing actions in the moment refers to making decisions based on immediate circumstances, while optimizing over time involves considering the long-term consequences of actions. Agents can use self-reflection to learn from past mistakes and improve reasoning skills. Algorithm Distillation encapsulates an algorithm in a long history-conditioned policy to predict actions leading to better performance than previous trials.\n"
     ]
    }
   ],
   "source": [
    "user_question = input(\"Please input your question to give to our RAG agent.\")\n",
    "rag_agent_response = rag_agent.invoke({\"question\": user_question})\n",
    "\n",
    "print(f\"Context: {rag_agent_response['context'][0].page_content}\")\n",
    "print(f\"Answer: {rag_agent_response['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5fe13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-agent-musings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
